<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>VGSL network specification &#8212; kraken  documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=89a84cc7" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="canonical" href="kraken.re/advanced/vgsl.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Training" href="../training/ketos.html" />
    <link rel="prev" title="Model Repository" href="repo.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="vgsl-network-specification">
<span id="vgsl"></span><h1>VGSL network specification<a class="headerlink" href="#vgsl-network-specification" title="Link to this heading">¶</a></h1>
<p>kraken implements a dialect of the Variable-size Graph Specification Language
(VGSL), enabling the specification of different network architectures for image
processing purposes using a short definition string.</p>
<section id="basics">
<h2>Basics<a class="headerlink" href="#basics" title="Link to this heading">¶</a></h2>
<p>A VGSL specification consists of an input block, one or more layers, and an
output block. For example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,48,0,1 Cr3,3,32 Mp2,2 Cr3,3,64 Mp2,2 S1(1x12)1,3 Lbx100 Do O1c103]</span>
</pre></div>
</div>
<p>The first block defines the input in order of [batch, height, width, channels]
with zero-valued dimensions being variable. Integer valued height or width
input specifications will result in the input images being automatically scaled
in either dimension.</p>
<p>When channels are set to 1 grayscale or B/W inputs are expected, 3 expects RGB
color images. Higher values in combination with a height of 1 result in the
network being fed 1 pixel wide grayscale strips scaled to the size of the
channel dimension.</p>
<p>After the input, a number of layers are defined. Layers operate on the channel
dimension; this is intuitive for convolutional layers but a recurrent layer
doing sequence classification along the width axis on an image of a particular
height requires the height dimension to be moved to the channel dimension,
e.g.:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,48,0,1 S1(1x48)1,3 Lbx100 O1c103]</span>
</pre></div>
</div>
<p>or using the alternative slightly faster formulation:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,1,0,48 Lbx100 O1c103]</span>
</pre></div>
</div>
<p>Finally an output definition is appended. When training sequence classification
networks with the provided tools the appropriate output definition is
automatically appended to the network based on the alphabet of the training
data.</p>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,1,0,48 Lbx100 Do 01c59]</span>

<span class="go">Creating new model [1,1,0,48 Lbx100 Do] with 59 outputs</span>
<span class="go">layer           type    params</span>
<span class="go">0               rnn     direction b transposed False summarize False out 100 legacy None</span>
<span class="go">1               dropout probability 0.5 dims 1</span>
<span class="go">2               linear  augmented False out 59</span>
</pre></div>
</div>
<p>A simple recurrent recognition model with a single LSTM layer classifying lines
normalized to 48 pixels in height.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,48,0,1 Cr3,3,32 Do0.1,2 Mp2,2 Cr3,3,64 Do0.1,2 Mp2,2 S1(1x12)1,3 Lbx100 Do 01c59]</span>

<span class="go">Creating new model [1,48,0,1 Cr3,3,32 Do0.1,2 Mp2,2 Cr3,3,64 Do0.1,2 Mp2,2 S1(1x12)1,3 Lbx100 Do] with 59 outputs</span>
<span class="go">layer           type    params</span>
<span class="go">0               conv    kernel 3 x 3 filters 32 activation r</span>
<span class="go">1               dropout probability 0.1 dims 2</span>
<span class="go">2               maxpool kernel 2 x 2 stride 2 x 2</span>
<span class="go">3               conv    kernel 3 x 3 filters 64 activation r</span>
<span class="go">4               dropout probability 0.1 dims 2</span>
<span class="go">5               maxpool kernel 2 x 2 stride 2 x 2</span>
<span class="go">6               reshape from 1 1 x 12 to 1/3</span>
<span class="go">7               rnn     direction b transposed False summarize False out 100 legacy None</span>
<span class="go">8               dropout probability 0.5 dims 1</span>
<span class="go">9               linear  augmented False out 59</span>
</pre></div>
</div>
<p>A model with a small convolutional stack before a recurrent LSTM layer. The
extended dropout layer syntax is used to reduce drop probability on the depth
dimension as the default is too high for convolutional layers. The remainder of
the height dimension (<cite>12</cite>) is reshaped into the depth dimensions before
applying the final recurrent and linear layers.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,0,0,3 Cr3,3,16 Mp3,3 Lfys64 Lbx128 Lbx256 Do 01c59]</span>

<span class="go">Creating new model [1,0,0,3 Cr3,3,16 Mp3,3 Lfys64 Lbx128 Lbx256 Do] with 59 outputs</span>
<span class="go">layer           type    params</span>
<span class="go">0               conv    kernel 3 x 3 filters 16 activation r</span>
<span class="go">1               maxpool kernel 3 x 3 stride 3 x 3</span>
<span class="go">2               rnn     direction f transposed True summarize True out 64 legacy None</span>
<span class="go">3               rnn     direction b transposed False summarize False out 128 legacy None</span>
<span class="go">4               rnn     direction b transposed False summarize False out 256 legacy None</span>
<span class="go">5               dropout probability 0.5 dims 1</span>
<span class="go">6               linear  augmented False out 59</span>
</pre></div>
</div>
<p>A model with arbitrary sized color image input, an initial summarizing
recurrent layer to squash the height to 64, followed by 2 bi-directional
recurrent layers and a linear projection.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">[1,1800,0,3 Cr3,3,32 Gn8 (I [Cr3,3,64,2,2 Gn8 CTr3,3,32,2,2]) Cr3,3,32 O2l8]</span>

<span class="go">layer           type    params</span>
<span class="go">0               conv    kernel 3 x 3 filters 32 activation r</span>
<span class="go">1               groupnorm       8 groups</span>
<span class="go">2               parallel        execute 2.0 and 2.1 in parallel</span>
<span class="go">2.0             identity</span>
<span class="go">2.1             serial  execute 2.1.0 to 2.1.2 in sequence</span>
<span class="go">2.1.0           conv    kernel 3 x 3 stride 2 x 2 filters 64 activation r</span>
<span class="go">2.1.1           groupnorm       8 groups</span>
<span class="go">2.1.2           transposed convolution  kernel 3 x 3 stride 2 x 2 filters 2 activation r</span>
<span class="go">3               conv    kernel 3 x 3 stride 1 x 1 filters 32 activation r</span>
<span class="go">4               linear  activation sigmoid</span>
</pre></div>
</div>
<p>A model that outputs heatmaps with 8 feature dimensions, taking color images with
height normalized to 1800 pixels as its input. It uses a strided convolution
to first scale the image down, and then a transposed convolution to transform
the image back to its original size. This is done in a parallel block, where the
other branch simply passes through the output of the first convolution layer.
The input of the last convolutional layer is then the output of the two branches
of the parallel block concatenated, i.e. the output of the first
convolutional layer together with the output of the transposed convolutional layer,
giving <cite>32 + 32 = 64</cite> feature dimensions.</p>
</section>
<section id="convolutional-layers">
<h2>Convolutional Layers<a class="headerlink" href="#convolutional-layers" title="Link to this heading">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">C[T][{name}](s|t|r|l|m)[{name}]&lt;y&gt;,&lt;x&gt;,&lt;d&gt;[,&lt;stride_y&gt;,&lt;stride_x&gt;][,&lt;dilation_y&gt;,&lt;dilation_x&gt;]</span>
<span class="go">s = sigmoid</span>
<span class="go">t = tanh</span>
<span class="go">r = relu</span>
<span class="go">l = linear</span>
<span class="go">m = softmax</span>
</pre></div>
</div>
<p>Adds a 2D convolution with kernel size <cite>(y, x)</cite> and <cite>d</cite> output channels, applying
the selected nonlinearity. Stride and dilation can be adjusted with the optional last
two parameters. <cite>T</cite> gives a transposed convolution. For transposed convolutions,
several output sizes are possible for the same configuration. The system
will try to match the output size of the different branches of parallel
blocks, however, this will only work if the transposed convolution directly
proceeds the confluence of the parallel branches, and if the branches with
fixed output size come first in the definition of the parallel block. Hence,
out of <cite>(I [Cr3,3,8,2,2 CTr3,3,8,2,2])</cite>, <cite>([Cr3,3,8,2,2 CTr3,3,8,2,2] I)</cite>
and <cite>(I [Cr3,3,8,2,2 CTr3,3,8,2,2 Gn8])</cite> only the first variant will
behave correctly.</p>
</section>
<section id="recurrent-layers">
<h2>Recurrent Layers<a class="headerlink" href="#recurrent-layers" title="Link to this heading">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">L[{name}](f|r|b)(x|y)[s][{name}]&lt;n&gt; LSTM cell with n outputs.</span>
<span class="go">G[{name}](f|r|b)(x|y)[s][{name}]&lt;n&gt; GRU cell with n outputs.</span>
<span class="go">f runs the RNN forward only.</span>
<span class="go">r runs the RNN reversed only.</span>
<span class="go">b runs the RNN bidirectionally.</span>
<span class="go">s (optional) summarizes the output in the requested dimension, return the last step.</span>
</pre></div>
</div>
<p>Adds either an LSTM or GRU recurrent layer to the network using either the <cite>x</cite>
(width) or <cite>y</cite> (height) dimension as the time axis. Input features are the
channel dimension and the non-time-axis dimension (height/width) is treated as
another batch dimension. For example, a <cite>Lfx25</cite> layer on an <cite>1, 16, 906, 32</cite>
input will execute 16 independent forward passes on <cite>906x32</cite> tensors resulting
in an output of shape <cite>1, 16, 906, 25</cite>. If this isn’t desired either run a
summarizing layer in the other direction, e.g. <cite>Lfys20</cite> for an input <cite>1, 1,
906, 20</cite>, or prepend a reshape layer <cite>S1(1x16)1,3</cite> combining the height and
channel dimension for an <cite>1, 1, 906, 512</cite> input to the recurrent layer.</p>
</section>
<section id="helper-and-plumbing-layers">
<h2>Helper and Plumbing Layers<a class="headerlink" href="#helper-and-plumbing-layers" title="Link to this heading">¶</a></h2>
<section id="max-pool">
<h3>Max Pool<a class="headerlink" href="#max-pool" title="Link to this heading">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Mp[{name}]&lt;y&gt;,&lt;x&gt;[,&lt;y_stride&gt;,&lt;x_stride&gt;]</span>
</pre></div>
</div>
<p>Adds a maximum pooling with <cite>(y, x)</cite> kernel_size and <cite>(y_stride, x_stride)</cite> stride.</p>
</section>
<section id="reshape">
<h3>Reshape<a class="headerlink" href="#reshape" title="Link to this heading">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">S[{name}]&lt;d&gt;(&lt;a&gt;x&lt;b&gt;)&lt;e&gt;,&lt;f&gt; Splits one dimension, moves one part to another</span>
<span class="go">        dimension.</span>
</pre></div>
</div>
<p>The <cite>S</cite> layer reshapes a source dimension <cite>d</cite> to <cite>a,b</cite> and distributes <cite>a</cite> into
dimension <cite>e</cite>, respectively <cite>b</cite> into <cite>f</cite>.  Either <cite>e</cite> or <cite>f</cite> has to be equal to
<cite>d</cite>. So <cite>S1(1, 48)1, 3</cite> on an <cite>1, 48, 1020, 8</cite> input will first reshape into
<cite>1, 1, 48, 1020, 8</cite>, leave the <cite>1</cite> part in the height dimension and distribute
the <cite>48</cite> sized tensor into the channel dimension resulting in a <cite>1, 1, 1024,
48*8=384</cite> sized output. <cite>S</cite> layers are mostly used to remove undesirable non-1
height before a recurrent layer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This <cite>S</cite> layer is equivalent to the one implemented in the tensorflow
implementation of VGSL, i.e. behaves differently from tesseract.</p>
</div>
</section>
</section>
<section id="regularization-layers">
<h2>Regularization Layers<a class="headerlink" href="#regularization-layers" title="Link to this heading">¶</a></h2>
<section id="dropout">
<h3>Dropout<a class="headerlink" href="#dropout" title="Link to this heading">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Do[{name}][&lt;prob&gt;],[&lt;dim&gt;] Insert a 1D or 2D dropout layer</span>
</pre></div>
</div>
<p>Adds an 1D or 2D dropout layer with a given probability. Defaults to <cite>0.5</cite> drop
probability and 1D dropout. Set to <cite>dim</cite> to <cite>2</cite> after convolutional layers.</p>
</section>
<section id="group-normalization">
<h3>Group Normalization<a class="headerlink" href="#group-normalization" title="Link to this heading">¶</a></h3>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Gn&lt;groups&gt; Inserts a group normalization layer</span>
</pre></div>
</div>
<p>Adds a group normalization layer separating the input into <cite>&lt;groups&gt;</cite> groups,
normalizing each separately.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="../index.html">
              <img class="logo" src="../_static/kraken.png" alt="Logo of kraken"/>
            </a></p>
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">VGSL network specification</a><ul>
<li><a class="reference internal" href="#basics">Basics</a></li>
<li><a class="reference internal" href="#examples">Examples</a></li>
<li><a class="reference internal" href="#convolutional-layers">Convolutional Layers</a></li>
<li><a class="reference internal" href="#recurrent-layers">Recurrent Layers</a></li>
<li><a class="reference internal" href="#helper-and-plumbing-layers">Helper and Plumbing Layers</a><ul>
<li><a class="reference internal" href="#max-pool">Max Pool</a></li>
<li><a class="reference internal" href="#reshape">Reshape</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regularization-layers">Regularization Layers</a><ul>
<li><a class="reference internal" href="#dropout">Dropout</a></li>
<li><a class="reference internal" href="#group-normalization">Group Normalization</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">Advanced Usage</a><ul>
      <li>Previous: <a href="repo.html" title="previous chapter">Model Repository</a></li>
      <li>Next: <a href="../training/ketos.html" title="next chapter">Training</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<h3>Versions</h3>
<ul>
  <li><a href="../../2.0.0/index.html">2.0.0</a></li>
  <li><a href="../../3.0/index.html">3.0</a></li>
  <li><a href="../../4.0/index.html">4.0</a></li>
  <li><a href="../../4.1/index.html">4.1</a></li>
  <li><a href="../../4.2.0/index.html">4.2.0</a></li>
  <li><a href="../../4.3.0/index.html">4.3.0</a></li>
  <li><a href="../../5.0.0/index.html">5.0.0</a></li>
  <li><a href="../../5.2/index.html">5.2</a></li>
  <li><a href="../../5.3.0/index.html">5.3.0</a></li>
  <li><a href="../../6.0.0/advanced/vgsl.html">6.0.0</a></li>
  <li><a href="vgsl.html">main</a></li>
</ul>


        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2015-2025, Benjamin Kiessling.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/advanced/vgsl.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>